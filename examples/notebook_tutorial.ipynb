{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Induction Head Detector Tutorial\n",
    "\n",
    "Learn to find and analyze induction heads in transformers!\n",
    "\n",
    "## What are Induction Heads?\n",
    "\n",
    "Induction heads are attention heads that implement in-context learning via:\n",
    "1. **Pattern Matching**: Find previous occurrence of current token\n",
    "2. **Copying**: Predict what came after that previous occurrence\n",
    "\n",
    "Example: `The cat sat on the mat. The cat` -> predicts `sat`\n",
    "\n",
    "## Why Do They Matter?\n",
    "\n",
    "Induction heads are believed to be the primary mechanism for:\n",
    "- In-context learning\n",
    "- Few-shot prompting\n",
    "- Pattern completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from induction_head_detector import InductionHeadDetector, DetectorConfig\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "seq_len = 64\n",
    "\n",
    "# Generate random attention patterns\n",
    "attention = np.random.rand(n_layers, n_heads, seq_len, seq_len)\n",
    "attention = attention / attention.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# Add synthetic induction head at layer 5, head 7\n",
    "for i in range(1, seq_len):\n",
    "    attention[5, 7, i, i-1] = 0.9  # Strong previous-token attention\n",
    "attention[5, 7] /= attention[5, 7].sum(axis=-1, keepdims=True)\n",
    "\n",
    "print(f\"Created attention patterns: {attention.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect Induction Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure detector\n",
    "config = DetectorConfig(\n",
    "    induction_threshold=0.3,\n",
    "    analyze_composition=True,\n",
    ")\n",
    "\n",
    "detector = InductionHeadDetector(config)\n",
    "\n",
    "# Detect induction heads\n",
    "heads = detector.detect(attention)\n",
    "\n",
    "print(f\"Found {len(heads)} induction heads:\")\n",
    "for head in heads[:5]:\n",
    "    print(f\"  Layer {head.layer}, Head {head.head}: score={head.score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Attention Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the induction head we created\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Induction head\n",
    "im1 = axes[0].imshow(attention[5, 7, :32, :32], cmap='viridis')\n",
    "axes[0].set_title('Layer 5, Head 7 (Induction Head)')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Random head for comparison\n",
    "im2 = axes[1].imshow(attention[0, 0, :32, :32], cmap='viridis')\n",
    "axes[1].set_title('Layer 0, Head 0 (Random)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Head Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the detected induction head\n",
    "if heads:\n",
    "    top_head = heads[0]\n",
    "    analysis = detector.analyze_head(\n",
    "        attention[top_head.layer, top_head.head],\n",
    "        layer=top_head.layer,\n",
    "        head=top_head.head,\n",
    "    )\n",
    "    \n",
    "    print(\"Induction Head Analysis:\")\n",
    "    print(f\"  Pattern type: {analysis.pattern_type}\")\n",
    "    print(f\"  Copying strength: {analysis.copying_strength:.3f}\")\n",
    "    print(f\"  Previous token attention: {analysis.prev_token_attention:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score All Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of induction scores\n",
    "scores = np.zeros((n_layers, n_heads))\n",
    "for head in heads:\n",
    "    scores[head.layer, head.head] = head.score\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(scores, cmap='Reds', aspect='auto')\n",
    "plt.colorbar(label='Induction Score')\n",
    "plt.xlabel('Head')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Induction Scores by Layer and Head')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial showed:\n",
    "1. How induction heads work (pattern matching + copying)\n",
    "2. Detecting induction heads via attention pattern analysis\n",
    "3. Visualizing and comparing attention patterns\n",
    "4. Scoring all heads for induction behavior\n",
    "\n",
    "For real models, use the full API to analyze actual attention weights!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
